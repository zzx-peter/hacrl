<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="In scenarios where multiple agents are to be optimized for the same task, vanilla Reinforcement Learning (RL) paradigms constrain each agent to execute tasks independently and update based solely on its own rollouts. They repeatedly generate trajectories and yield verifiable rewards, In scenarios where multiple agents are to be optimized for the same task, vanilla Reinforcement Learning (RL) paradigms constrain each agent to execute tasks independently and update based solely on its own rollouts, they repeatedly generate trajectories and yield verifiable rewards, while these costly intermediate results are only utilized for self-training.
        To break through this wasteful practice, we introduce HACPO (Heterogeneous Agent Collaborative Policy Optimization) to achieve this goal. It is a novel collaborative multi-agent RL training paradigm that enables rollout sharing,  maximizing rollout utilization and facilitating mutual knowledge transfer among heterogeneous agents.
        To effectively bridge the discrepancies in capability and policy distribution, we introduce four tailored modifications and rigorously prove the unbiasedness of these improvements in advantage estimation and the validity of the optimization direction through theoretical analysis.
        Experimental results demonstrate that HACPO consistently achieves superior accuracy across various heterogeneous model combinations and datasets. 
        This work points to a promising new direction for efficient collaborative learning among multi-agent systems.">
  <meta name="keywords" content="LLM, Reasoning, Multi-Agent Reinforcement Learning, Heterogeneous Agents, Collaborative Optimization, Rollout Sharing">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Heterogeneous Agent Collaborative Reinforcement Learning</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body, {delimiters: [{left: '$$', right: '$$', display: true}, {left: '$', right: '$', display: false}]});"></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <style>
    :root {
      --primary: #363636;
      --primary-light: #4a4a4a;
      --text-dark: #1f2937;
      --text-muted: #6b7280;
      --bg-white: #ffffff;
      --bg-gray: #f9fafb;
      --border-light: #e5e7eb;
      --sage-red: #ab1727;
      --sage-blue: #2563eb;
    }
    
    body { color: var(--text-dark); }
    
    .hero { background: #f3f4f6; border-bottom: 1px solid #e5e7eb; overflow: visible !important; }
    .hero .hero-body { padding-top: 2rem; padding-bottom: 0.8rem; overflow: visible !important; }
    .hero * { overflow: visible !important; }
    .hero .container, .hero .columns, .hero .column { overflow: visible !important; max-height: none !important; }
    .hero .title.is-1 { color: var(--sage-red); font-weight: 400; font-family: 'Google Sans', 'Times New Roman', serif; }
    .hero .publication-authors a { color: #374151 !important; }
    .hero .publication-authors a:hover { color: var(--sage-red) !important; text-decoration: underline; }
    .hero .publication-authors span { color: #4b5563; }
    
    .publication-links .button {
      margin: 0.25rem;
      background: transparent;
      border: 1px solid #d1d5db;
      color: #374151;
    }
    .publication-links .button:hover {
      background: #f3f4f6;
      border-color: #9ca3af;
      color: #1f2937;
    }

    h2.section-title, h3.section-title, h4.subsection-title, .title.section-title {
      color: var(--primary) !important;
      font-weight: 600;
      font-family: 'Google Sans', sans-serif;
      margin-bottom: 1.5rem;
    }
    
    h4.subsection-title {
      font-size: 1.25rem;
      font-family: 'Google Sans', sans-serif;
      margin-top: 2rem;
      margin-bottom: 1rem;
    }
    
    .content p, .content li {
      font-family: 'Noto Sans', sans-serif;
      font-size: 1rem;
      line-height: 1.7;
    }
    
    .section-white { background: var(--bg-white); }
    .section-gray { background: var(--bg-gray); }
    
    .insight-box {
      background: #fafafa;
      border: 1px solid var(--border-light);
      border-left: 4px solid #9ca3af;
      padding: 1.5rem 2rem;
      border-radius: 4px;
      margin: 1.5rem 0;
    }
    .insight-box p {
      font-family: 'Castoro', Georgia, serif;
      font-size: 1.1rem;
    }
    
    .math-comparison {
      display: grid;
      grid-template-columns: 1fr 1fr;
      gap: 1.5rem;
      margin: 2rem 0;
    }
    @media (max-width: 768px) { .math-comparison { grid-template-columns: 1fr; } }
    
    .math-box {
      background: #fff;
      border-radius: 6px;
      padding: 1.5rem;
      border: 1px solid var(--border-light);
    }
    .math-box h4 { font-weight: 600; margin-bottom: 1rem; color: var(--text-dark); }
    
    .figure-container {
      background: transparent;
      border-radius: 0;
      padding: 1rem 0;
      border: none;
      margin: 1.5rem 0;
    }
    .figure-container img { border-radius: 4px; }
    .figure-caption { font-size: 0.9rem; color: var(--text-muted); margin-top: 1rem; text-align: center; }
    
    .result-card {
      background: #fff;
      border-radius: 8px;
      padding: 2rem;
      margin: 1.5rem 0;
      border: 1px solid var(--border-light);
    }
    .result-card h4 { color: var(--text-dark); font-weight: 600; margin-bottom: 1rem; }
    
    .formula-box {
      background: transparent;
      border: none;
      padding: 1rem 0;
      margin: 1.5rem 0;
      text-align: center;
    }
    
    .conclusion-box {
      background: #fff;
      border: 1px solid var(--border-light);
      border-radius: 8px;
      padding: 2rem;
    }
    
    .bibtex-box { 
      background: #374151;
      border: 1px solid #4b5563;
      border-radius: 8px; 
      padding: 1.5rem;
    }
    .bibtex-box code {
      font-family: 'JetBrains Mono', monospace;
      font-size: 0.85rem;
      color: #e5e7eb;
    }

    footer.footer { background: #1f2937; color: #9ca3af; padding: 2rem; }
    footer a { color: #93c5fd; }
  </style>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-widescreen">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Heterogeneous Agent Collaborative Reinforcement Learning</h1>
          
          <div class="is-size-5 publication-authors">
            <span class="author-block"><a href="mailto:22376220@buaa.edu.cn">Zhixia Zhang</a><sup>1*</sup></span><span class="author-block">, </span>
            <span class="author-block">Zixuan Huang<sup>1 2*</sup></span><span class="author-block">, </span>
            <span class="author-block">Xin Xia<sup>2</sup></span><span class="author-block">, </span>
            <span class="author-block">Deqing Wang<sup>1</sup></span><span class="author-block">, </span>
            <span class="author-block">Fuzhen Zhuang<sup>1</sup></span><span class="author-block">, </span>
            <span class="author-block">Shuai Ma<sup>1</sup></span><span class="author-block">, </span>
            <span class="author-block">Ning Ding<sup>3</sup></span><span class="author-block">, </span>
            <span class="author-block">Yaodong Yang<sup>4</sup></span><span class="author-block">, </span>
            <span class="author-block">Jianxin Li<sup>1</sup></span><span class="author-block">, </span>
            <span class="author-block"><a href="mailto:yikunb@buaa.edu.cn">Yikun Ban</a><sup>1†</sup></span>
          </div>
          <div class="is-size-6 publication-authors" style="margin-top: 0.4rem;">
            <span class="author-block"><sup>1</sup>Beihang University</span><span class="author-block"> &nbsp;</span>
            <span class="author-block"><sup>2</sup>Bytedance China</span><span class="author-block"> &nbsp;</span>
            <span class="author-block"><sup>3</sup>Tsinghua University</span><span class="author-block"> &nbsp;</span>
            <span class="author-block"><sup>4</sup>Peking University</span><br>
            <span class="author-block" style="margin-left: 0.8em;"><sup>*</sup>Equal Contribution</span>
            <span class="author-block" style="margin-left: 0.8em;"><sup>†</sup>Corresponding Authors</span>
          </div>

          <div class="has-text-centered" style="margin-top: 1.2rem;">
            <!-- contact us for code -->
            <!-- 在 HTML 中添加模态框结构 -->
            <div id="customAlert" class="modal" style="display: none; position: fixed; z-index: 1000; left: 0; top: 0; width: 100%; height: 100%; background-color: rgba(0,0,0,0.4);">
                <div style="background-color: #fefefe; margin: 15% auto; padding: 20px; border: 1px solid #888; width: 300px; border-radius: 5px; box-shadow: 0 4px 8px rgba(0,0,0,0.2);">
                <div style="font-weight: bold; margin-bottom: 15px; color: #ab1727;">Contact Information</div>
                <div style="margin-bottom: 20px;">Please contact 22376220@buaa.edu.cn</div>
                <button onclick="document.getElementById('customAlert').style.display='none'" style="background-color: #2563eb; color: white; border: none; padding: 8px 16px; border-radius: 4px; cursor: pointer;">OK</button>
                </div>
            </div>
            
            <!-- 修改 Code 按钮 -->
            <span class="link-block"> 
                <a href="#" onclick="document.getElementById('customAlert').style.display='block'; return false;" class="external-link button is-normal is-rounded"> 
                <span class="icon"><i class="fab fa-github"></i></span> 
                <span>Code</span> 
                </a> 
            </span>
          </div>

          <div style="margin-top: 1.2rem; text-align: center;">
            <p style="font-family: 'Castoro', Georgia, serif; font-size: 1.05rem; color: #374151; max-width: 800px; margin: 0 auto;">
              <strong>TL;DR</strong>&nbsp; HACPO is a novel collaborative reinforcement learning framework that enables heterogeneous agents to share rollouts.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section section-white" style="padding-top: 0.8rem; padding-bottom: 0.5rem;">
  <div class="container is-max-desktop">
    
    <div class="figure-container" style="margin: 0;">
      <center>
        <img src="./static/images/main.png" alt="Workflow of HACPO" style="max-width: 100%;">
      </center>
      <p class="figure-caption">
        <strong>Figure 1.</strong> In HACPO, shared rollouts from multiple heterogeneous agents are leveraged for collaborative training. Built upon vanilla RL Optimization, HACPO introduces four algorithmic innovations to mitigate capability and policy distribution discrepancy.
      </p>
    </div>
    
    <div class="content has-text-justified" style="margin-top: 2rem;">
      <h2 class="title is-3 section-title">Abstract</h2>
      <p>
        In scenarios where multiple agents are to be optimized for the same task, vanilla Reinforcement Learning (RL) paradigms constrain each agent to execute tasks independently and update based solely on its own rollouts. They repeatedly generate trajectories and yield verifiable rewards, In scenarios where multiple agents are to be optimized for the same task, vanilla Reinforcement Learning (RL) paradigms constrain each agent to execute tasks independently and update based solely on its own rollouts, they repeatedly generate trajectories and yield verifiable rewards, while these costly intermediate results are only utilized for self-training.
        To break through this wasteful practice, we introduce <strong>HACPO</strong> (Heterogeneous Agent Collaborative Policy Optimization) to achieve this goal. It is a novel collaborative multi-agent RL training paradigm that enables rollout sharing,  maximizing rollout utilization and facilitating mutual knowledge transfer among heterogeneous agents.
        To effectively bridge the discrepancies in capability and policy distribution, we introduce four tailored modifications and rigorously prove the unbiasedness of these improvements in advantage estimation and the validity of the optimization direction through theoretical analysis.
        Experimental results demonstrate that HACPO consistently achieves superior accuracy across various heterogeneous model combinations and datasets. 
        This work points to a promising new direction for efficient collaborative learning among multi-agent systems.
      </p>
    </div>
  </div>
</section>

<!--defination of heterogeneous agent collaborative reinforcement learning-->
<section class="section section-gray">
  <div class="container is-max-desktop">
    <h2 class="title is-3 section-title">The Novel Problem: Heterogeneous Agent Collaborative Reinforcement Learning</h2>
    <div class="figure-container" style="margin: 0;">
      <!-- <center>
        <img src="./static/images/HACPO_MARL_KD.png" alt="HACRL vs MARL vs KD" style="max-width: 100%;">
      </center> -->
      <center>
        <img src="./static/images/HACPO_MARL_KD_1.png" alt="HACRL vs MARL vs KD" style="max-width: 60%;">
      </center>
      <p class="figure-caption">
        <strong>Figure 2.</strong> The significant differences among Multi-Agent RL, Knowledge Distillation, and the proposed HACRL. HACRL targets independent execution with collaborative optimization.
      </p>
    </div>
    <div class="content has-text-justified">
      <p>
        Typically, given one identical task, multiple agents execute RLVR optimization independently of one another.For essentially the same objective, they repeatedly generate trajectories and yield verifiable rewards, while these costly intermediate results are only utilized for self-training. We first formalize this setting as <strong>Heterogeneous Agent Collaborative Reinforcement Learning (HACRL)</strong>, which captures collaborative policy optimization among heterogeneous agents that execute independently at inference time. 
      </p>
      <p>
        HACRL differs fundamentally from LLM-based multi-agent reinforcement learning (MARL), which trains agents to coordinate and jointly solve tasks through interaction.It is also distinct from knowledge distillation (KD), where a typically fixed teacher transfers knowledge to a student. In contrast, HACRL focuses on independent execution with collaborative optimization: agents share verified rollouts during training, yet operate independently at inference time without coordination.
      </p>
      <div class="insight-box">
        <p>
          <strong>Definition of HACRL Problem:</strong> We consider the Heterogeneous Agent Collaborative Reinforcement Learning (HACRL) framework with n LLM agents.
        </p>
        <p>
          During training, for a query $q \sim \mathcal D$, each agent $k$ independently samples $G$ candidate responses from its policy:
          $$Y_k(q) = \{y_{k,1}, \dots, y_{k,G}\} \sim \pi_{\theta_k}(\cdot \mid q).$$
          The corresponding verifiable rewards are:
          $$ \mathcal R_k(q) = \{ R(y_{k,i}) \mid i = 1,\dots,G \}.$$
          The objective is to optimize each agent $k \in \{1, \dots, n\}$ by maximizing:
          $$J^{(k)} = J^{(k)}_{homo}(Y_k(q), R_k(q)) + J^{(k)}_{hete}(\{Y_j(q), R_j(q)\}_{j \neq k})$$
          where $J^{(k)}_{homo}$ is computed using rollouts generated by agent $k$ itself, and $J^{(k)}_{hete}$ leverages rollouts generated by other heterogeneous agents.
        </p>
      </div>
      <p>
        We categorize heterogeneity among distinct LLM agents into three types:
      </p>
      <ul>
        <li><strong>Heterogeneous State:</strong> Agents differ only in optimization state.</li>
        <li><strong>Heterogeneous Size:</strong> Agents share architecture but differ in parameter size.</li>
        <li><strong>Heterogeneous Model:</strong> Agents differ in architecture, tokenizer, and training objectives.</li>
      </ul>
    </div>
  </div>
</section>

<!--core insight of HACPO: 1.Agent-Capability-Aware Advantage Estimation 2.Model Capabilities Discrepancy Coefficient 3.Exponential Importance Sampling 4.Stepwise Clipping-->
<section class="section section-white">
  <div class="container is-max-desktop">
    <h2 class="title is-3 section-title">Heterogeneous Agent Collaborative Policy Optimization (HACPO)</h2>
    <div class="content has-text-justified">
      <p>
        To solve the HACRL problem, we propose Heterogeneous Agent Collaborative Policy Optimization (HACPO), which introduces four tailored modifications to bridge capability and distribution discrepancies between heterogeneous agents.
      </p>

      <h4 class="subsection-title">1. Agent-Capability-Aware Advantage Estimation</h4>
      <p>
        We propose a capability-aware estimator that assign distinct inter-group advantage baseline for each agent based on their relative performance.
      </p>
      <div class="math-box">
        In training step $t$, the advantage of the $i$-th response for agent $k$ is:
        $$A^{(k)}_{t,i} = \frac{R(y^{(k)}_{t,i}) - \hat{\mu}^{(k)}_t}{\sigma_{t,joint}}$$
        <p class="is-size-6 mt-2">
          The baseline $\hat{\mu}^{(k)}_t$ is computed by:
          $$\hat{\mu}^{(k)}_t = \frac{1}{nG} \sum_{j=1}^n \sum_{i=1}^G \omega^{(k,j)}_t R(y^{(j)}_{t,i})$$
          where $\omega^{(k,j)}_t = \hat{P}^{(k)}_t / \hat{P}^{(j)}_t$ is the capability ratio ($\hat{P}^{(k)}_t$ is the smoothed accuracy of agent $k$ at step $t$).
        </p>
      </div>

      <h4 class="subsection-title">2. Model Capabilities Discrepancy Coefficient</h4>
      <p>
        To encourage learning from stronger agents while being conservative with weaker ones, we use the capability ratio to modulate the effective advantage.
      </p>
      <div class="math-box">
        $$\tilde{A}^{(k)}_{t,i} = \begin{cases} A^{(k)}_{t,i} & y^{(k)}_{t,i} \in D^{(k)}_t \\ \omega^{(j,k)}_t A^{(j)}_{t,i} & y^{(j)}_{t,i} \in D^{(j)}_t, j \neq k \end{cases}$$
      </div>

      <h4 class="subsection-title">3. Exponential Importance Sampling</h4>
      <p>
        To correct for distributional mismatches without being overly aggressive, we introduce a non-gradient exponential reweighting to the importance sampling ratio.
      </p>
      <div class="math-box">
        $$\tilde{s}^{(k,j)}_{t,i} = s^{(k,j)}_{t,i} \cdot (\text{sg}[s^{(k,j)}_{t,i}])^\alpha$$
        <p class="is-size-6 mt-2">
          where $\alpha \geq 0$ controls the degree of conservativeness.
        </p>
      </div>

      <h4 class="subsection-title">4. Stepwise Clipping</h4>
      <p>
        Cross-agent importance sampling ratios fluctuate irregularly both across steps and within a step. We first apply an asymmetric clipping bounds for cross-agent responses to ensure the cross-agent responses only be downweighted, but never upweighted. Then, we apply a stepwise clipping strategy to prevent cross-agent rollouts from dominating late-stage updates within a batch, thereby improving training stability.
      </p>
      <div class="math-box">
        $$\text{clip}(s^{(k,j)}_{t,i}) = \text{clip}(s^{(k,j)}_{t,i}, 1 - \delta + k \cdot \delta_{step}, 1.0)$$
        $k$ denote the number of parameter updates performed so far within the current step, and $\delta_{\mathrm{step}}$ denote the per-update tightening factor.
      </div>
    </div>
  </div>
</section>

<!--Experimental Results: 1.main results 2.ablation studies-->
<section class="section section-gray">
  <div class="container is-max-desktop">
    <h2 class="title is-3 section-title">Experimental Results</h2>
    
    <div class="content has-text-justified">
      <h3 class="title is-4">Experimental Setup & Baselines</h3>
      <p>
        We evaluate HACPO on 7.5k high-quality math questions from the MATH dataset and test on seven challenging benchmarks. To rigorously validate the effectiveness of our collaborative paradigm, we compare HACPO against three distinct types of baselines:
      </p>
      <ul>
        <li><strong>Standard Single-Agent Baselines (GRPO, GSPO):</strong> Establish benchmarks for isolated training performance.</li>
        <li><strong>Resource-Equivalent Baseline (GSPO×2):</strong> A single-agent setting with double rollouts and updates. This rules out the impact of increased data volume, verifying that gains come from collaboration rather than just more compute.</li>
        <li><strong>Naive Collaborative Baseline (Naive):</strong> A multi-agent setting with simple rollout sharing but lacking our specific algorithmic innovations (Capability-Aware Estimation, Discrepancy Coefficient, etc.).</li>
      </ul>

      <h3 class="title is-4">Main Results</h3>
      <p>
        HACPO demonstrates consistent and superior performance improvements across three distinct heterogeneity settings:
      </p>
      <ul>
        <li><strong>Heterogeneous State (Qwen3-4B + Qwen3-4B-Instruct):</strong> Even when agents differ only by optimization state, HACPO enables the stronger agent (Instruct) to benefit from the weaker one's complementary exploration signals (alternative reasoning paths and informative errors).</li>
        <li><strong>Heterogeneous Size (Qwen3-1.7B-Base + Qwen3-4B-Base):</strong> Both models improve significantly. The smaller model serves as a distinct explorer, facilitating bidirectional knowledge transfer that boosts the larger model's performance beyond self-training limits.</li>
        <li><strong>Heterogeneous Model (Qwen3-4B-Base + Llama3.2-3B-Instruct):</strong> Despite substantial differences in architecture, tokenizers, and training objectives, HACPO successfully extracts transferable knowledge from cross-model rollouts, improving both agents.</li>
      </ul>

      <div class="result-card has-text-centered">
        <img src="./static/images/main_table.png" alt="Table 1: Main results across three heterogeneity settings" style="max-width: 100%; margin-bottom: 1rem;">
        <p class="figure-caption"><strong>Table 1.</strong> Main results across three heterogeneity settings. We compare our method against Standard Single-Agent Baselines (GRPO, GSPO), a Resource-Equivalent Baseline (GSPO×2) and a Naive multi-agent rollout share baseline(Naive).</p>
        <br>
        <img src="./static/images/HACPO vs GSPO.png" alt="Figure 3: Training curves of GSPO and HACPO" style="max-width: 100%; margin-bottom: 1rem;">
        <p class="figure-caption"><strong>Figure 3.</strong> Training curves of GSPO and HACPO. HACPO demonstrates faster convergence and higher final performance compared to single-agent baselines.</p>
      </div>

      <h3 class="title is-4">Ablation Studies</h3>
      <p>
        We validate the necessity of each component through extensive ablation studies.
      </p>
      
      <div class="result-card">
        <!-- 1. Agent-Capability-Aware Advantage Estimation -->
        <div class="content">
            <p><strong>Agent-Capability-Aware Advantage Estimation:</strong> Removing this module leads to performance degradation due to systematic bias in advantage estimation caused by capability discrepancies.</p>
            <div class="has-text-centered">
                <img src="./static/images/Ablation of Adv.png" alt="Table 2: Ablation of Advantage Estimator" style="max-width: 60%; margin-top: 0.5rem;">
                <p class="figure-caption"><strong>Table 2.</strong> Ablation of Advantage Estimator</p>
            </div>
        </div>
        <hr>

        <!-- 2. Model Capabilities Discrepancy Coefficient -->
        <div class="content">
            <p><strong>Model Capabilities Discrepancy Coefficient:</strong> Essential for modulating gradients—amplifying signals from stronger agents while attenuating noise from weaker ones.</p>
            <div class="has-text-centered">
                <img src="./static/images/Ablation of Model Capabilities Discrepancy Coefficient.png" alt="Table 3: Ablation of Model Capabilities Discrepancy Coefficient" style="max-width: 60%; margin-top: 0.5rem;">
                <p class="figure-caption"><strong>Table 3.</strong> Ablation of Model Capabilities Discrepancy Coefficient</p>
            </div>
        </div>
        <hr>

        <!-- 3. Exponential Importance Sampling -->
        <div class="content">
            <p><strong>Exponential Importance Sampling:</strong> Examining the impact of different values of $\alpha$ on the performance of HACPO.</p>
            <div class="has-text-centered">
                <img src="./static/images/impact of Exponential Importance Sampling.png" alt="Table 4: Impact of Exponential Importance Sampling" style="max-width: 60%; margin-top: 0.5rem;">
                <p class="figure-caption"><strong>Table 4.</strong> Impact of Exponential Importance Sampling ($\alpha$)</p>
            </div>
        </div>
        <hr>

        <!-- 4. Stepwise Clipping -->
        <div class="content">
            <p><strong>Stepwise Clipping:</strong> Crucial for stabilizing collaborative learning. Without it, high-variance cross-agent responses can destabilize the training process.</p>
            <div class="has-text-centered">
                <img src="./static/images/Ablation of stepwise clipping.png" alt="Figure 4: The Ablation of Stepwise Clipping" style="max-width: 80%; margin-top: 0.5rem;">
                <p class="figure-caption"><strong>Figure 4.</strong> The Ablation of Stepwise Clipping. Removing clipping or the stepwise schedule leads to instability or suboptimal convergence.</p>
            </div>
        </div>

      </div>
    </div>
  </div>
</section>

<footer class="footer">
  <div class="container is-max-desktop">
    <div class="columns">
      <div class="column is-8">
        <p style="margin-bottom: 0;">
          &copy; 2026 Anonymous Institution. All rights reserved.
        </p>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
