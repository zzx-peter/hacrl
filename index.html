<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Recent advancements in large reasoning models (LRMs) have greatly improved their capabilities on complex reasoning tasks through long Chains of Thought (CoTs). However, this approach often results in substantial redundancy, impairing computational efficiency and causing significant delays in real-time applications. Recent studies show that longer reasoning chains are frequently uncorrelated with correctness and can even be detrimental to accuracy. In a further in-depth analysis of this phenomenon, we surprisingly uncover and empirically verify that LRMs implicitly know the appropriate time to stop thinking, while this capability is obscured by current sampling paradigms. Motivated by this, we introduce SAGE (Self-Aware Guided Efficient Reasoning), a novel sampling paradigm that unleashes this efficient reasoning potential. Furthermore, integrating SAGE as mixed sampling into groupbased reinforcement learning (SAGE-RL) effectively incorporates SAGE-discovered efficient reasoning patterns into standard pass@1 inference, markedly enhancing both the reasoning accuracy and efficiency of LRMs across multiple challenging mathematical benchmarks.">
  <meta name="keywords" content="LLM, Reasoning, Efficient Reasoning, SAGE, Reinforcement Learning, Chain of Thought">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Does Your Reasoning Model Implicitly Know When to Stop Thinking?</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <!-- KaTeX for math rendering -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body, {delimiters: [{left: '$$', right: '$$', display: true}, {left: '$', right: '$', display: false}]});"></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <style>
    :root {
      --primary: #363636;
      --primary-light: #4a4a4a;
      --text-dark: #1f2937;
      --text-muted: #6b7280;
      --bg-white: #ffffff;
      --bg-gray: #f9fafb;
      --border-light: #e5e7eb;
      --sage-red: #ab1727;
      --sage-blue: #2563eb;
    }
    
    body { color: var(--text-dark); }
    
    .hero { background: #f3f4f6; border-bottom: 1px solid #e5e7eb; overflow: visible !important; }
    .hero .hero-body { padding-top: 2rem; padding-bottom: 0.8rem; overflow: visible !important; }
    .hero * { overflow: visible !important; }
    .hero .container, .hero .columns, .hero .column { overflow: visible !important; max-height: none !important; }
    .hero .title.is-1 { color: var(--sage-red); font-weight: 400; font-family: Georgia, 'Times New Roman', serif; }
    .hero .publication-authors a { color: #374151 !important; }
    .hero .publication-authors a:hover { color: var(--sage-red) !important; text-decoration: underline; }
    .hero .publication-authors span { color: #4b5563; }
    
    .publication-links .button {
      margin: 0.25rem;
      background: transparent;
      border: 1px solid #d1d5db;
      color: #374151;
    }
    .publication-links .button:hover {
      background: #f3f4f6;
      border-color: #9ca3af;
      color: #1f2937;
    }

    /* Section titles */
    h2.section-title,
    h3.section-title,
    h4.subsection-title,
    .title.section-title {
      color: var(--primary) !important;
      font-weight: 600;
      font-family: 'Google Sans', sans-serif;
      margin-bottom: 1.5rem;
    }
    
    h4.subsection-title {
      font-size: 1.25rem;
      font-family: 'Google Sans', sans-serif;
      margin-top: 2rem;
      margin-bottom: 1rem;
    }
    
    /* Body text */
    .content p, .content li {
      font-family: 'Noto Sans', sans-serif;
      font-size: 1rem;
      line-height: 1.7;
    }
    
    .section-white { background: var(--bg-white); }
    .section-gray { background: var(--bg-gray); }
    
    .insight-box {
      background: #fafafa;
      border: 1px solid var(--border-light);
      border-left: 4px solid #9ca3af;
      padding: 1.5rem 2rem;
      border-radius: 4px;
      margin: 1.5rem 0;
    }
    .insight-box p {
      font-family: 'Castoro', Georgia, serif;
      font-size: 1.1rem;
    }
    
    .math-comparison {
      display: grid;
      grid-template-columns: 1fr 1fr;
      gap: 1.5rem;
      margin: 2rem 0;
    }
    @media (max-width: 768px) { .math-comparison { grid-template-columns: 1fr; } }
    
    .math-box {
      background: #fff;
      border-radius: 6px;
      padding: 1.5rem;
      border: 1px solid var(--border-light);
    }
    .math-box.rl { border-top: 3px solid #dc2626; }
    .math-box.ml { border-top: 3px solid var(--primary); }
    .math-box h4 { font-weight: 600; margin-bottom: 1rem; color: var(--text-dark); }
    
    .figure-container {
      background: transparent;
      border-radius: 0;
      padding: 1rem 0;
      border: none;
      margin: 1.5rem 0;
    }
    .figure-container img { border-radius: 4px; }
    .figure-caption { font-size: 0.9rem; color: var(--text-muted); margin-top: 1rem; text-align: center; }
    
    /* Equal height for analysis figures */
    .analysis-columns {
      display: flex;
      gap: 1.5rem;
      align-items: stretch;
    }
    .analysis-columns .analysis-col {
      flex: 1;
      display: flex;
    }
    .analysis-columns .figure-container {
      flex: 1;
      display: flex;
      flex-direction: column;
      margin: 0;
    }
    .analysis-columns .figure-container .img-wrapper {
      flex: 1;
      display: flex;
      align-items: center;
      justify-content: center;
    }
    .analysis-columns .figure-container img {
      max-width: 100%;
      max-height: 280px;
      object-fit: contain;
    }
    @media (max-width: 768px) {
      .analysis-columns { flex-direction: column; }
    }
    
    .result-card {
      background: #fff;
      border-radius: 8px;
      padding: 2rem;
      margin: 1.5rem 0;
      border: 1px solid var(--border-light);
    }
    .result-card h4 { color: var(--text-dark); font-weight: 600; margin-bottom: 1rem; }
    
    .highlight-stat {
      background: var(--primary);
      color: #fff;
      padding: 0.2rem 0.6rem;
      border-radius: 4px;
      font-weight: 600;
      font-size: 0.9rem;
    }
    .highlight-stat.sage { background: var(--sage-blue); }
    
    .algorithm-box {
      background: #1f2937;
      color: #e5e7eb;
      border-radius: 6px;
      padding: 1.5rem;
      font-family: 'JetBrains Mono', monospace;
      font-size: 0.85rem;
      overflow-x: auto;
    }
    .algorithm-box .comment { color: #9ca3af; }
    .algorithm-box .keyword { color: #f472b6; }
    .algorithm-box .function { color: #60a5fa; }
    
    table.comparison-table {
      width: 100%;
      border-collapse: collapse;
      margin: 1.5rem 0;
      background: #fff;
      border: 1px solid var(--border-light);
      border-radius: 6px;
      overflow: hidden;
    }
    table.comparison-table th {
      background: var(--primary);
      color: #fff !important;
      padding: 0.75rem 1rem;
      font-weight: 600;
      text-align: left;
    }
    table.comparison-table th * { color: #fff !important; }
    table.comparison-table th .katex, table.comparison-table th .katex * { color: #fff !important; }
    table.comparison-table td { padding: 0.75rem 1rem; border-bottom: 1px solid var(--border-light); }
    table.comparison-table tr:last-child td { border-bottom: none; }
    table.comparison-table tr:hover td { background: var(--bg-gray); }
    
    pre code { display: block; padding: 1rem; background: #1f2937; color: #e5e7eb; border-radius: 6px; overflow-x: auto; }
    .bibtex-box { 
      background: #374151;
      border: 1px solid #4b5563;
      border-radius: 8px; 
      padding: 1.5rem;
    }
    .bibtex-box pre {
      margin: 0;
      background: transparent;
    }
    .bibtex-box code {
      font-family: 'JetBrains Mono', 'Fira Code', 'Monaco', monospace;
      font-size: 0.85rem;
      line-height: 1.6;
      color: #e5e7eb;
      background: transparent;
      padding: 0;
    }
    
    footer.footer { background: #1f2937; color: #9ca3af; padding: 2rem; overflow: hidden; }
    footer.footer .columns { margin: 0; }
    footer.footer .container { overflow: hidden; }
    footer a { color: #93c5fd; }
    footer a:hover { color: #bfdbfe; }
    
    .emoji-icon { margin-right: 0.5rem; }
    .section { padding: 3rem 1.5rem; }
    
    .formula-box {
      background: transparent;
      border: none;
      padding: 1rem 0;
      margin: 1.5rem 0;
      text-align: center;
    }
    
    
    .conclusion-box {
      background: #fff;
      border: 1px solid var(--border-light);
      border-radius: 8px;
      padding: 2rem;
    }
    .conclusion-box p {
      line-height: 1.8;
      margin-bottom: 1rem;
    }
    .conclusion-box p:last-child {
      margin-bottom: 0;
    }
    
    /* ==================== MOBILE RESPONSIVE STYLES ==================== */
    
    /* Tablets and smaller */
    @media (max-width: 1024px) {
      .hero .title.is-1 { font-size: 2.2rem; }
    }
    
    /* Mobile devices */
    @media (max-width: 768px) {
      /* Hero Section */
      .hero .hero-body { padding-top: 1.2rem; padding-bottom: 0.6rem; }
      .hero .title.is-1 { 
        font-size: 1.5rem; 
        white-space: normal !important; 
        line-height: 1.3;
      }
      
      /* Authors - make them wrap nicely */
      .publication-authors { font-size: 0.9rem !important; }
      .publication-authors .author-block { 
        display: inline; 
        white-space: nowrap;
      }
      .is-size-6.publication-authors { font-size: 0.8rem !important; }
      .is-size-6.publication-authors .author-block { margin-left: 0.3em !important; }
      
      /* Section padding */
      .section { padding: 2rem 1rem; }
      
      /* Titles */
      h2.title.is-3, .title.is-3.section-title { font-size: 1.4rem; }
      h4.subsection-title { font-size: 1.1rem; }
      
      /* Math comparison boxes */
      .math-comparison { gap: 1rem; }
      .math-box { padding: 1rem; }
      .math-box p { font-size: 0.95rem; }
      
      /* Formula boxes - handle overflow */
      .formula-box { 
        overflow-x: auto; 
        padding: 0.5rem;
      }
      .formula-box p { font-size: 0.9rem; }
      
      /* Insight boxes */
      .insight-box { padding: 1rem 1.25rem; }
      .insight-box p { font-size: 1rem; }
      
      /* Result cards */
      .result-card { padding: 1.25rem; }
      .result-card h4 { font-size: 1.1rem; }
      
      /* Figure captions */
      .figure-caption { font-size: 0.8rem; }
      
      /* Tables - make scrollable */
      .comparison-table-wrapper {
        overflow-x: auto;
        -webkit-overflow-scrolling: touch;
      }
      table.comparison-table { 
        min-width: 500px; 
        font-size: 0.85rem;
      }
      table.comparison-table th, 
      table.comparison-table td { 
        padding: 0.5rem 0.75rem; 
      }
      
      /* Conclusion box */
      .conclusion-box { padding: 1.25rem; }
      
      /* BibTeX */
      .bibtex-box { padding: 1rem; }
      .bibtex-box code { font-size: 0.7rem; }
      
      /* Publication links buttons */
      .publication-links .button { 
        font-size: 0.85rem;
        padding: 0.4rem 0.8rem;
      }
      
      /* Footer */
      footer.footer { padding: 1.5rem 1rem; }
      footer.footer img { height: 60px !important; }
      footer p { font-size: 0.85rem; }
      
      /* TL;DR text */
      .hero .hero-body p[style*="Castoro"] { font-size: 0.95rem !important; }
    }
    
    /* Small phones */
    @media (max-width: 480px) {
      .hero .title.is-1 { font-size: 1.25rem; }
      .publication-authors { font-size: 0.8rem !important; }
      
      h2.title.is-3, .title.is-3.section-title { font-size: 1.25rem; }
      
      .content p, .content li { font-size: 0.9rem; }
      
      .math-box h4 { font-size: 0.95rem; }
      .math-box p { font-size: 0.85rem; }
      
      .result-card { padding: 1rem; }
      .result-card h4 { font-size: 1rem; }
      
      /* Extra small formulas */
      .formula-box p { font-size: 0.85rem; }
      
      /* Theorem boxes */
      .result-card[style*="border-left"] { padding: 1rem; }
      .result-card[style*="border-left"] h4 { font-size: 1rem; }
      .result-card[style*="border-left"] div[style*="text-align: center"] { font-size: 0.9rem !important; }
    }
    
    /* Ensure images never overflow */
    img { max-width: 100%; height: auto; }
    
    /* KaTeX formula overflow handling */
    .katex-display { overflow-x: auto; overflow-y: hidden; }
    
    /* Global overflow prevention */
    .container { overflow-x: hidden; }
    .content { word-wrap: break-word; overflow-wrap: break-word; }
    
    /* Table wrapper for mobile scrolling */
    .comparison-table-wrapper {
      width: 100%;
      overflow-x: auto;
      -webkit-overflow-scrolling: touch;
    }
    
    /* Algorithm box mobile */
    @media (max-width: 768px) {
      .algorithm-box { 
        font-size: 0.75rem; 
        padding: 1rem;
      }
    }
    
    /* Result card theorem boxes mobile adjustments */
    @media (max-width: 768px) {
      .result-card div[style*="text-align: center"] {
        overflow-x: auto;
        -webkit-overflow-scrolling: touch;
      }
    }
  </style>
</head>
<body>

<!-- Hero Section -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-widescreen">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Does Your Reasoning Model Implicitly Know When to Stop Thinking?</h1>
          
          <!-- Authors Section -->
          <div class="is-size-5 publication-authors">
            <span class="author-block"><a href="mailto:huangzx@buaa.edu.cn">Zixuan Huang</a><sup>1,2</sup></span>,
            <span class="author-block">Xin Xia<sup>2</sup></span>,
            <span class="author-block">Yuxi Ren<sup>2</sup></span>,
            <span class="author-block">Jianbin Zheng<sup>2</sup></span>,
            <span class="author-block">Xuanda Wang<sup>2</sup></span>,
            <span class="author-block">Zhixia Zhang<sup>1*</sup></span>,
            <span class="author-block">Hongyan Xie<sup>1</sup></span>,<br>
            <span class="author-block">Songshi Liang<sup>2</sup></span>,
            <span class="author-block">Zehao Chen<sup>1</sup></span>,
            <span class="author-block">Xuefeng Xiao<sup>2</sup></span>,
            <span class="author-block">Fuzhen Zhuang<sup>1</sup></span>,
            <span class="author-block">Jianxin Li<sup>1</sup></span>,
            <span class="author-block"><a href="mailto:yikunb@buaa.edu.cn">Yikun Ban</a><sup>1†</sup></span>,
            <span class="author-block"><a href="mailto:dqwang@buaa.edu.cn">Deqing Wang</a><sup>1†</sup></span>
          </div>
          <div class="is-size-6 publication-authors" style="margin-top: 0.4rem;">
            <span class="author-block"><sup>1</sup>Beihang University</span>
            <span class="author-block" style="margin-left: 0.8em;"><sup>2</sup>Bytedance China</span>
            <span class="author-block" style="margin-left: 0.8em;"><sup>*</sup>Second Student Author</span>
            <span class="author-block" style="margin-left: 0.8em;"><sup>†</sup>Co-corresponding Authors</span>
          </div>

          <div class="has-text-centered" style="margin-top: 0.8rem;">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2602.08354" class="external-link button is-normal is-rounded" target="_blank">
                  <span class="icon"><i class="ai ai-arxiv"></i></span>
                  <span>arXiv</span>
                </a>
              </span>

              <!-- contact us for code -->
            <!-- 在 HTML 中添加模态框结构 -->
            <div id="customAlert" class="modal" style="display: none; position: fixed; z-index: 1000; left: 0; top: 0; width: 100%; height: 100%; background-color: rgba(0,0,0,0.4);">
                <div style="background-color: #fefefe; margin: 15% auto; padding: 20px; border: 1px solid #888; width: 300px; border-radius: 5px; box-shadow: 0 4px 8px rgba(0,0,0,0.2);">
                <div style="font-weight: bold; margin-bottom: 15px; color: #ab1727;">Contact Information</div>
                <div style="margin-bottom: 20px;">Please contact huang_zx@buaa.edu.cn</div>
                <button onclick="document.getElementById('customAlert').style.display='none'" style="background-color: #2563eb; color: white; border: none; padding: 8px 16px; border-radius: 4px; cursor: pointer;">OK</button>
                </div>
            </div>
            
            <!-- 修改 Code 按钮 -->
            <span class="link-block"> 
                <a href="#" onclick="document.getElementById('customAlert').style.display='block'; return false;" class="external-link button is-normal is-rounded"> 
                <span class="icon"><i class="fab fa-github"></i></span> 
                <span>Code</span> 
                </a> 
            </span>


              <span class="link-block">
                <a href="https://arxiv.org/pdf/2602.08354v1" class="external-link button is-normal is-rounded" target="_blank">
                  <span class="icon"><i class="ai ai-pdf"></i></span>
                  <span>Paper</span>
                </a>
              </span>
            </div>
          </div>

          <div style="margin-top: 0.6rem; text-align: center;">
            <p style="font-family: 'Castoro', Georgia, serif; font-size: 1.05rem; color: #374151;">
              <strong>TL;DR</strong>&nbsp; Large Reasoning Models (LRMs) implicitly know when to stop thinking—we unlock this ability with SAGE, a self-aware sampling paradigm, and SAGE-RL, which integrates efficient reasoning patterns into standard inference for <span class="highlight-stat sage">+2.1% avg accuracy</span> and <span class="highlight-stat sage">44.1% fewer tokens</span> on mathematical benchmarks.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- ==================== ABSTRACT & TEASER ==================== -->
<section class="section section-white" style="padding-top: 0.8rem; padding-bottom: 0.5rem;">
  <div class="container is-max-desktop">
    
    <!-- Teaser Diagram -->
    <div class="figure-container" style="margin: 0;">
      <center>
        <img src="./static/images/main.png" alt="SAGE Teaser" style="max-width: 100%;">
      </center>
      <p class="figure-caption">
        <strong>SAGE Unleashes Efficient Reasoning Potential.</strong> SAGE uncovers the optimal concise reasoning chains hidden in pass@k that are obscured by standard pass@1 sampling. SAGE-RL integrates these efficient patterns into LRMs, achieving higher accuracy with far fewer tokens across challenging mathematical benchmarks (AIME, MATH-500, OlympiadBench, AMC23, etc.).
      </p>
    </div>
    
    <!-- Abstract -->
    <div class="content has-text-justified" style="margin-top: 2rem;">
      <h2 class="title is-3 section-title">Abstract</h2>
      <p>
        Recent advancements in large reasoning models (LRMs) have greatly improved their capabilities on complex reasoning tasks through long Chains of Thought (CoTs). However, this approach often results in substantial redundancy, impairing computational efficiency and causing significant delays in real-time applications. Recent studies show that longer reasoning chains are frequently uncorrelated with correctness and can even be detrimental to accuracy. In a further in-depth analysis of this phenomenon, we surprisingly uncover and empirically verify that LRMs <strong>implicitly know the appropriate time to stop thinking</strong>, while this capability is obscured by current sampling paradigms.
      </p>
      <p>
        Motivated by this insight, we introduce <strong>SAGE (Self-Aware Guided Efficient Reasoning)</strong>, a novel sampling paradigm that unleashes this efficient reasoning potential by leveraging the model's self-confidence to discover precise, concise reasoning chains. Furthermore, integrating SAGE as mixed sampling into group-based reinforcement learning (<strong>SAGE-RL</strong>) effectively incorporates SAGE-discovered efficient reasoning patterns into standard pass@1 inference. Extensive experiments on six challenging mathematical benchmarks (MATH-500, AIME 2024/2025, AMC23, OlympiadBench, Minerva) show that SAGE-RL markedly enhances both the reasoning accuracy and efficiency of LRMs, achieving an average <strong>+2.1% accuracy gain</strong> and <strong>44.1% token reduction</strong> compared to state-of-the-art baselines.
      </p>
    </div>
  </div>
</section>

<!-- ==================== THE PROBLEM: REDUNDANT REASONING ==================== -->
<section class="section section-gray">
  <div class="container is-max-desktop">
    <h2 class="title is-3 section-title">The Problem: Redundant Chain of Thought Reasoning</h2>
    
    <div class="content has-text-justified">
      <p>
        Modern LRMs rely on long Chain of Thought (CoT) reasoning to solve complex mathematical and logical problems, enabled by Reinforcement Learning from Verifiable Rewards (RLVR) algorithms like GRPO and GSPO that incentivize "thinking longer". While this boosts performance on hard tasks, it introduces severe <strong>reasoning redundancy</strong>: models often continue generating steps long after deriving the correct answer, wasting computational resources and increasing latency.
      </p>
      
      <div class="result-card" style="margin-top: 1.5rem;">
        <h4>Key Observations on Redundant Reasoning</h4>
        <ul style="margin-left: 1.5rem; line-height: 1.8;">
          <li>Longer CoTs do not correlate with correctness: On AIME 2025, DeepSeek-R1 produces responses <strong>5× longer</strong> than Claude 3.7 Sonnet with comparable accuracy.</li>
          <li>Shorter CoTs often outperform longer ones: QwQ-32B's shortest responses on AIME/HMMT achieve <strong>+2% accuracy</strong> with <strong>31% fewer tokens</strong> than random samples.</li>
          <li>72% of AIME 2025 problems with both correct/incorrect answers have <strong>longer responses as incorrect</strong> (Shrivastava et al., 2025).</li>
        </ul>
      </div>
      
      <!-- RFCS Metric -->
      <h4 class="subsection-title">Ratio of the First Correct Step (RFCS)</h4>
      <p>
        To quantify this redundancy, we define the <strong>RFCS</strong> metric: the step index of the first correct answer divided by the total number of reasoning steps. For over half of MATH-500 samples across all tested LRMs (DS-1.5B, DeepScaleR, Qwen3-8B), RFCS &lt; 1—meaning the model finds the correct answer early but continues reasoning unnecessarily.
      </p>
      
      <div class="figure-container">
        <center><img src="./static/images/rfcs-1.png" alt="RFCS Statistics on MATH-500" style="max-width: 50%;"></center>
        <p class="figure-caption">
          <strong>RFCS Statistics on MATH-500.</strong> All LRMs show significant ineffective reasoning steps (RFCS(&lt;1) &gt; 80%) and low average RFCS (~0.6), demonstrating widespread overthinking under standard pass@1 sampling.
        </p>
      </div>
      
      <!-- Example of Redundant Reasoning -->
      <div class="figure-container">
        <center><img src="./static/images/case_study-1.png" alt="Redundant Reasoning Example" style="max-width: 50%;"></center>
        <p class="figure-caption">
          <strong>Redundant Reasoning Example.</strong> The model derives the correct answer in 500 tokens but generates an additional 452 redundant tokens of double-checking—this is typical of LRMs under standard sampling.
        </p>
      </div>
      
      <div class="insight-box">
        <p style="margin: 0;">
          Current sampling paradigms (pass@1) obscure the LRM's inherent ability to recognize when to stop thinking. The model <em>knows</em> the optimal stopping point—it just can't act on it under standard inference.
        </p>
      </div>
    </div>
  </div>
</section>

<!-- ==================== CORE INSIGHT: LRMS IMPLICITLY KNOW WHEN TO STOP ==================== -->
<section class="section section-white">
  <div class="container is-max-desktop">
    <h2 class="title is-3 section-title">Core Insight: LRMs Implicitly Know When to Stop Thinking</h2>
    
    <div class="content has-text-justified">
      <p>
        Through systematic experiments with token-wise reasoning path exploration, we uncover three critical observations that confirm LRMs have an inherent sense of optimal reasoning termination:
      </p>
      
      <!-- Observation 1 -->
      <div class="result-card" style="border-left: 4px solid var(--sage-blue); padding: 1.5rem;">
        <h4 style="color: var(--sage-blue); margin-bottom: 0.5rem;">Observation 1: High-Confidence Paths Are Short & Effective</h4>
        <p>
          Using a cumulative log-probability score $\Phi$ to track high-confidence reasoning paths, we find that increasing exploration width leads to <strong>consistent response length reduction AND accuracy improvement</strong>. In contrast, paths selected by next-token probability ($\phi$) suffer rapid accuracy degradation (length collapse).
        </p>
        <!-- <div class="figure-container" style="margin-top: 1rem; margin-bottom: 0;">
          <center><img src="./static/images/sort_or_not-1.png" alt="Φ vs φ Performance" style="max-width: 50%;"></center>
        </div> -->


        <div class="figure-container" style="margin-top: 1rem; margin-bottom: 0;"> 
            <div style="display: flex; justify-content: center; gap: 1rem; width: 100%;">
              <!-- 左侧 PDF -->
              <div style="flex: 1; max-width: 45%;">
                <img src="./static/images/sort_or_not-1.png" width="100%" height="400px" style="border: none;">
              </div>
              <!-- 右侧 PDF -->
              <div style="flex: 1; max-width: 45%;">
                <img src="./static/images/sort_or_not_table.png" width="100%" height="400px" style="border: none;">
              </div>
            </div>
        </div>

      </div>
      
      <!-- Observation 2 -->
      <div class="result-card" style="border-left: 4px solid var(--sage-blue); padding: 1.5rem; margin-top: 1.5rem;">
        <h4 style="color: var(--sage-blue); margin-bottom: 0.5rem;">Observation 2: High-Confidence Paths Have Confident Ends</h4>
        <p>
          The termination token ($$) consistently ranks <strong>1st</strong> in high-confidence paths (selected by $\Phi$) when it appears, meaning the model is highly confident in stopping. For next-token selected paths, the termination token's rank increases with exploration—showing uncertainty. Greedy/random sampling misses these short, high-confidence chains.
        </p>
        <div class="figure-container" style="margin-top: 1rem; margin-bottom: 0;"> 
            <div style="display: flex; justify-content: center; gap: 1rem; width: 100%;">
              <!-- 左侧 PDF -->
              <div style="flex: 1; max-width: 45%;">
                <img src="./static/images/eot_rank_ratio-1.png" width="100%" height="400px" style="border: none;">
              </div>
              <!-- 右侧 PDF -->
              <div style="flex: 1; max-width: 45%;">
                <img src="./static/images/core_insight-1.png" width="100%" height="400px" style="border: none;">
              </div>
            </div>
        </div>
      </div>
      
      <!-- Observation 3 -->
      <div class="result-card" style="border-left: 4px solid var(--sage-blue); padding: 1.5rem; margin-top: 1.5rem;">
        <h4 style="color: var(--sage-blue); margin-bottom: 0.5rem;">Observation 3: Scaling Exploration Drives Capability Convergence</h4>
        <p>
          As exploration width increases, LRMs converge to <strong>higher accuracy with shorter responses</strong>, with token efficiency (accuracy/length) rising sharply. This convergence is universal across models (DS-1.5B, DeepScaleR) and datasets (MATH-500, AMC23), proving the model's inherent efficient reasoning capability.
        </p>
        <div class="figure-container" style="margin-top: 1rem; margin-bottom: 0;">
            <center><img src="./static/images/SAGE_dif_BW-1.png" alt="Capability Convergence" style="max-width: 80%;"></center>
        </div>
        <div class="figure-container" style="margin-top: 1rem; margin-bottom: 0;">
          <center><img src="./static/images/token_efficiency-1.png" alt="Capability Convergence" style="max-width: 50%;"></center>
        </div>
      </div>
      
      <div class="insight-box" style="margin-top: 2rem;">
        <p style="margin: 0; font-size: 1.15rem;">
          <strong>Key Takeaway:</strong> LRMs have an innate ability to identify concise, correct reasoning chains—this capability is simply locked by current sampling paradigms that prioritize next-token probability over cumulative confidence.
        </p>
      </div>
    </div>
  </div>
</section>

<!-- ==================== SAGE: SELF-AWARE GUIDED EFFICIENT REASONING ==================== -->
<section class="section section-gray">
  <div class="container is-max-desktop">
    <h2 class="title is-3 section-title">SAGE: Self-Aware Guided Efficient Reasoning</h2>
    
    <div class="content has-text-justified">
      <p>
        Building on our core insight, we design <strong>SAGE</strong>—a simple, training-free sampling paradigm that unlocks the LRM's implicit efficient reasoning capability by leveraging <strong>cumulative self-confidence ($\Phi$)</strong> to discover concise, correct reasoning chains. SAGE replaces token-level exploration with <strong>step-wise reasoning chain exploration</strong> and uses the model's inherent confidence for automatic termination.
      </p>
      
      <!-- Cumulative Confidence Score Φ -->
      <h4 class="subsection-title">Cumulative Confidence Score $\Phi$</h4>
      <p>
        SAGE's core is the <strong>average cumulative log-probability score</strong> that measures the model's confidence in an entire reasoning chain (not just the next token):
      </p>
      <!-- 关键问题在于 \mathbf{y}_{<i} 中的 < 字符。在 HTML 中，< 是一个特殊字符，用于开始 HTML 标签，浏览器会尝试将其解析为 HTML 标签的开始，而不是数学公式的一部分。 -->
      <div class="formula-box"> 
            <p style="font-size: 1.1rem; margin: 0;"> 
                $$\Phi(\mathbf{y}_{\le k})=\frac{1}{k} \sum_{i=1}^{k} \log \pi_\theta \bigl( y_i \mid \mathbf{y}_{\lt i}, \mathbf{x} \bigr)$$ 
            </p> 
    </div>
      <p>
        Where $y_{\leq k}$ is the reasoning chain up to step $k$, $\phi$ is the next-token log-probability, and $\pi_\theta$ is the model's policy. This score captures the model's overall confidence in a reasoning path—critical for identifying high-quality, concise chains.
      </p>
      
      <!-- SAGE Key Components -->
      <h4 class="subsection-title">SAGE Key Components</h4>
      <div class="math-comparison">
        <div class="math-box" style="border-top: 3px solid var(--sage-blue);">
          <h4 style="color: var(--sage-blue); text-align: center;">Step-Wise Exploration</h4>
          <p>
            Extends candidate sequences by <strong>full reasoning steps</strong> (not individual tokens) using random sampling, maintaining top-$m$ high-confidence sequences via $\Phi$. This aligns with human reasoning (step-by-step) and avoids token-level noise.
          </p>
          <p style="text-align: center; margin-top: 1rem; font-size: 1rem;">
            $y_{\leq i}^{(j, k)}=y_{\leq i-1}^{(j)} \oplus r_{i}^{(j, k)}$
          </p>
        </div>
        <div class="math-box" style="border-top: 3px solid var(--sage-blue);">
          <h4 style="color: var(--sage-blue); text-align: center;">Confidence-Based Termination</h4>
          <p>
            Automatically terminates reasoning when a candidate sequence ends with $$—no manual tolerance parameters needed. High-confidence paths lead to confident termination, so the model's own signal dictates stopping.
          </p>
          <p style="text-align: center; margin-top: 1rem; font-size: 1rem;">
            Add to $O$ if $r_{k}^{(j, k)}$ ends with the eot token
          </p>
        </div>
      </div>
      
      <!-- SAGE Inference Scaling -->
      <h4 class="subsection-title">SAGE Inference Scaling</h4>
      <p>
        SAGE outperforms degraded SAGE (greedy step sampling) across all step budgets, with two key scaling properties:
      </p>
      <ul style="margin-left: 1.5rem; line-height: 1.8;">
        <li><strong>Constrained budgets</strong>: SAGE stops earlier with more complete CoTs, outperforming degraded SAGE in accuracy with similar length.</li>
        <li><strong>Ample budgets</strong>: SAGE uncovers superior reasoning chains (shorter, more accurate) with a stable performance gap.</li>
        <li><strong>Model/dataset adaptation</strong>: Boosts <strong>accuracy</strong> for strong models/hard datasets (DeepScaleR, AMC23) and <strong>efficiency</strong> for weaker models/simple datasets (DS-1.5B, MATH-500).</li>
      </ul>
      
      <div class="figure-container">
        <center><img src="./static/images/SAGE_perform-1.png" alt="SAGE Inference Scaling" style="max-width: 80%;"></center>
        <p class="figure-caption">
          <strong>SAGE Inference Scaling on MATH-500 & AMC23.</strong> SAGE consistently outperforms Degrade-SAGE across step budgets, with larger gains on harder datasets (AMC23) and stronger models (DeepScaleR).
        </p>
      </div>
      
      <!-- SAGE vs Beam Search
      <h4 class="subsection-title">SAGE vs Vanilla Beam Search</h4>
      <p>
        While SAGE builds on beam search principles, it differs fundamentally: beam search discards high-confidence termination paths in later steps, while SAGE <strong>accepts termination immediately</strong> when the model signals confidence. SAGE achieves <strong>+7% accuracy</strong> with <strong>33% fewer tokens</strong> than beam search on DS-1.5B (MATH-500 subset).
      </p> -->
    </div>
  </div>
</section>

<!-- ==================== SAGE-RL: INTEGRATING EFFICIENT REASONING PATTERNS ==================== -->
<section class="section section-white">
  <div class="container is-max-desktop">
    <h2 class="title is-3 section-title">SAGE-RL: Integrating Efficient Reasoning into Standard Inference</h2>
    
    <div class="content has-text-justified">
      <p>
        SAGE unlocks efficient reasoning during inference, but to <strong>incorporate these patterns into the model's core policy</strong> for standard pass@1 inference, we introduce <strong>SAGE-RL</strong>—a minimal modification to RLVR (GRPO/GSPO) that uses SAGE as <strong>mixed sampling</strong> in the rollout phase.
      </p>
      
      <!-- SAGE-RL Core Idea -->
      <div class="result-card" style="border-left: 4px solid var(--sage-red);">
        <h4 style="color: var(--sage-red); margin-bottom: 0.8rem;">SAGE-RL Core Idea</h4>
        <p>
          For each query, RLVR typically samples $G$ responses via random sampling. SAGE-RL modifies this to:
        </p>
        <ol style="margin-left: 1.5rem; line-height: 1.8;">
          <li>Generate $r$ responses with <strong>SAGE (m,r)</strong> (high-quality, efficient reasoning chains).</li>
          <li>Generate the remaining $G-r$ responses with <strong>standard random sampling</strong>.</li>
          <li>Use the combined set for RLVR advantage estimation and policy update.</li>
        </ol>
        <p style="margin-top: 1rem;">
          This is a <strong>one-line modification</strong> to existing RLVR implementations—no changes to the reward function, optimization objective, or training pipeline.
        </p>
      </div>
      
      <!-- SAGE-RL Objectives -->
      <h4 class="subsection-title">SAGE-RL Optimization Objectives</h4>
      <p>
        SAGE-RL retains the original GRPO/GSPO objectives but splits the rollout set into SAGE and random samples. For SAGE-GRPO, the objective is:
      </p>
      <div class="formula-box">
        <p style="font-size: 0.95rem; margin: 0; line-height: 1.8;">
          $$\mathcal{J}_{SAGE-GRPO }(\theta)=\mathbb{E}\left[\frac{1}{G}\left(\underbrace{\sum_{i=1}^{r} \frac{1}{|y_{i}|} \sum_{t=1}^{|y_{i}|} min \left(w_{i,t}\hat{A}_{i,t}, clip(w_{i,t}) \hat{A}_{i,t}\right)}_{SAGE(m,r)}+\underbrace{\sum_{i=r+1}^{G} \frac{1}{|y_{i}|} \sum_{t=1}^{|y_{i}|} min \left(w_{i,t}\hat{A}_{i,t}, clip(w_{i,t}) \hat{A}_{i,t}\right)}_{Random Sampling }\right)\right]$$
        </p>
      </div>
      <p>
        Where $w_{i,t}$ is the token-level importance ratio, $\hat{A}_{i,t}$ is the advantage, and $G=8$, $r=2$ (our default) balances efficiency and exploration.
      </p>
      
      <!-- SAGE-RL Training Dynamics -->
      <h4 class="subsection-title">SAGE-RL Training Dynamics</h4>
      <p>
        SAGE-RL exhibits distinct training behavior compared to vanilla RLVR:
      </p>
      <ul style="margin-left: 1.5rem; line-height: 1.8;">
        <li><strong>Faster accuracy gain</strong>: Pass@1 rises more rapidly and converges to a higher value.</li>
        <li><strong>Continuous length reduction</strong>: Response length decreases throughout training (no plateau).</li>
        <li><strong>Greater entropy reduction</strong>: Policy becomes more confident in efficient reasoning chains.</li>
        <li><strong>Higher KL divergence</strong>: Policy shifts from the original distribution to adopt SAGE's efficient patterns.</li>
      </ul>
      
      <div class="figure-container">
        <center><img src="./static/images/train_dynamic.png" alt="SAGE-RL Training Dynamics" style="max-width: 90%;"></center>
        <!-- <p class="figure-caption">
          <strong>SAGE-RL Performance Scaling with Problem Difficulty.</strong> SAGE-RL achieves larger accuracy gains on harder benchmarks (AIME 2025, OlympiadBench) compared to easier ones (MATH-500), demonstrating its effectiveness on challenging reasoning tasks.
        </p> -->
      </div>

      <div class="figure-container">
        <center><img src="./static/images/main_table_all.png" alt="SAGE-RL Training Dynamics" style="max-width: 90%;"></center>
        <p class="figure-caption">
          <strong>SAGE-RL Performance Scaling with Problem Difficulty.</strong> SAGE-RL achieves larger accuracy gains on harder benchmarks (AIME 2025, OlympiadBench) compared to easier ones (MATH-500), demonstrating its effectiveness on challenging reasoning tasks.
        </p>
      </div>

      
    </div>
  </div>
</section>

<!-- ==================== ABALATION STUDIES ==================== -->
<section class="section section-white">
  <div class="container is-max-desktop">
    <h2 class="title is-3 section-title">Hyperparameters Sensitivity Analysis</h2>
    <h4 class="subsection-title">Impact on Computational Cost </h4>
    <div class="content has-text-justified">
        <!-- 我们测试了在不同的探索宽度下SAGE的运行时延以及SAGE-RL tuned models的近似运行时延。 -->
        <p>
            We tested the running latency of SAGE and the approximate running latency of SAGE-RL tuned models under different exploration widths.
        </p>
          
        <div class="figure-container">
        <center><img src="./static/images/computation.png" alt="SAGE Sampling Ratio Ablation" style="max-width: 80%;"></center>
        <!-- <p class="figure-caption">
            <strong>Ablation on SAGE Sampling Ratio (G=8).</strong> $r=2$ (25% SAGE samples) achieves the highest token efficiency, balancing efficient reasoning patterns with exploration.
        </p> -->
        </div>
    
        <p>
            Once the exploration width exceeds 2, the growth rate of inference time accelerates further. Therefore, we primarily set exploration width $m$ = 2, which represents the transition
            point between the slow-growth and fast-growth regions, to achieve a balanced trade-off between efficiency and performance.
        </p>
    </div>


    <h4 class="subsection-title">Impact on SAGE-RL Performance</h4>
    <div class="content has-text-justified">
      <p>
        Through experiments, we investigated the impact of different hyperparameter combinations on SAGE-RL performance and obtained the following key findings:
      </p>
      
    <!-- Ablation 1: Optimal SAGE Sampling Ratio -->
      <div class="figure-container">
        <center><img src="./static/images/ab_dynamic.png" alt="SAGE Sampling Ratio Ablation" style="max-width: 80%;"></center>
        <!-- <p class="figure-caption">
          <strong>Ablation on SAGE Sampling Ratio (G=8).</strong> $r=2$ (25% SAGE samples) achieves the highest token efficiency, balancing efficient reasoning patterns with exploration.
        </p> -->
      </div>
      <div class="figure-container">
        <center><img src="./static/images/ab_table.png" alt="SAGE Sampling Ratio Ablation" style="max-width: 80%;"></center>
        <!-- <p class="figure-caption">
          <strong>Ablation on SAGE Sampling Ratio (G=8).</strong> $r=2$ (25% SAGE samples) achieves the highest token efficiency, balancing efficient reasoning patterns with exploration.
        </p> -->
      </div>

    <p>
    When the number of rollouts per group  $r$  increases from 1 to 2, the model performance improvement is limited with minimal impact on policy updates, as rollouts with similar reasoning trajectories cannot provide additional valid information.
    </p>
    <p>
    When the exploration width $m$ increases from 1 to 2, the model achieves significant improvements in both performance and efficiency. A limited $m$ causes SAGE-RL's optimization behavior to approach standard GRPO, confirming the critical role of exploration width in activating the model's efficient reasoning capability.
    </p>
    <p>
    Among different hyperparameter combinations, SAGE (2,2)-GRPO demonstrates the optimal overall performance, representing a favorable balance between performance and exploration efficiency.
    </p>
      

    </div>
  </div>
</section>

<!-- ==================== CONCLUSION ==================== -->
<section class="section section-gray">
  <div class="container is-max-desktop">
    <h2 class="title is-3 section-title">Conclusion</h2>
    
    <div class="content has-text-justified">
      <p>
        In this work, we make a surprising discovery: <strong>Large Reasoning Models (LRMs) implicitly know when to stop thinking</strong>. This capability is obscured by current sampling paradigms that prioritize next-token probability over cumulative confidence, leading to redundant reasoning and inefficient computation.
      </p>
      
      <p>
        To unlock this potential, we introduce <strong>SAGE</strong>, a self-aware sampling paradigm that leverages cumulative self-confidence to discover concise, correct reasoning chains. We further integrate SAGE into reinforcement learning via <strong>SAGE-RL</strong>, a minimal modification to RLVR that incorporates efficient reasoning patterns into the model's core policy.
      </p>
      
      <div class="conclusion-box">
        <p>
          <strong>Key Contributions:</strong>
        </p>
        <ul style="margin-left: 1.5rem; line-height: 1.8;">
          <li><strong>Novel Insight:</strong> LRMs have an innate ability to identify optimal stopping points in reasoning chains, which is locked by current sampling methods.</li>
          <li><strong>SAGE:</strong> A training-free sampling paradigm that unlocks efficient reasoning by leveraging cumulative self-confidence for automatic termination.</li>
          <li><strong>SAGE-RL:</strong> A minimal RLVR modification that integrates efficient reasoning patterns into standard pass@1 inference with a one-line change.</li>
          <li><strong>State-of-the-Art Results:</strong> +2.1% average accuracy gain and 44.1% token reduction across six challenging mathematical benchmarks, outperforming all baselines.</li>
        </ul>
        <p style="margin-top: 1rem;">
          SAGE and SAGE-RL provide a new path toward efficient reasoning in LRMs, demonstrating that models can achieve both higher accuracy and lower computational cost by simply learning to trust their own confident reasoning chains.
        </p>
      </div>
    </div>
  </div>
</section>

<!-- ==================== BIBTEX ==================== -->
<section class="section section-white" style="padding-top: 2rem; padding-bottom: 2rem;">
  <div class="container is-max-desktop">
    <h2 class="title is-3 section-title">BibTeX</h2>
    
    <div class="content has-text-justified">
      <div class="bibtex-box">
        <pre><code>@inproceedings{huang2026sage,
  title={Does Your Reasoning Model Implicitly Know When to Stop Thinking?},
  author={Huang, Zixuan and Xia, Xin and Ren, Yuxi and Zheng, Jianbin and Wang, Xuanda and Zhang, Zhixia and Xie, Hongyan and Liang, Songshi and Chen, Zehao and Xiao, Xuefeng and Zhuang, Fuzhen and Li, Jianxin and Ban, Yikun and Wang, Deqing},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2026}
}</code></pre>
      </div>
    </div>
  </div>
</section>

<!-- ==================== FOOTER ==================== -->
<footer class="footer">
  <div class="container is-max-desktop">
    <div class="columns">
      <div class="column is-8">
        <!-- <p style="margin-bottom: 0;">
          <strong>SAGE: Self-Aware Guided Efficient Reasoning</strong>
        </p> -->
        <!-- <p style="margin-bottom: 0.5rem;">
          <a href="https://arxiv.org/abs/2602.05811">arXiv</a> | 
          <a href="https://github.com/yourusername/sage-rl">GitHub</a> | 
          <a href="./static/papers/sage_icml2026-1.png">Paper</a>
        </p> -->
        <p style="margin-bottom: 0;">
          &copy; 2026 Beihang University & Bytedance China. All rights reserved.
        </p>
      </div>
      <!-- <div class="column is-4 has-text-right">
        <img src="./static/images/buaa.png" alt="Beihang University Logo" style="height: 80px;">
      </div> -->
    </div>
  </div>
</footer>

</body>
</html>



